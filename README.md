# Real-Time Data Pipeline Platform

An interactive platform to ingest, process, and analyze real-time web logs using Kafka, Spark Structured Streaming, and Delta Lake. The repository includes scripts to run the end-to-end pipeline, optional ML-based anomaly detection, and a simple server for UI streaming.

## Key Features

### Real-Time Updates

  - **Live Log Stream**: Real-time view of logs generated by the pipeline
  - **Dynamic Metrics**: Automatic refresh of metrics based on real data
  - **Interactive Charts**: Charts update on new incoming data
  - **SSE Connection**: Server-Sent Events for instant updates

### Real Pipeline Data

  - **Kafka Integration**: Direct connection to Kafka topics
  - **Spark SQL Queries**: Execute queries against Delta Lake data
  - **Delta Lake Analytics**: Query Delta tables directly
  - **No Mock Data**: All data comes from the real pipeline

### Interactive Interface

  - **Connection Status**: Visual indicators of pipeline health
  - **Query Builder**: UI to run custom Spark SQL queries
  - **Advanced Filters**: Filter logs by level, source, endpoint
  - **Anomaly Detection**: Automatic anomaly detection on data

## Architecture

```
      Web Logs (Sources)
              |
              v
        Kafka (Broker)
              |
              v
  Spark Streaming (Processing)
              |
              v
      Delta Lake (Storage)
              |
              v
      API Server (Express)
              |
              v
    Web Platform (React App)
```

## Singleton Spark Session

The platform manages Spark with a singleton `SparkSession` (see `spark_session_manager.py`) to avoid multiple `SparkContext` errors (SPARK-2243).  

All pipeline components use `get_spark()` to share the same session, ensuring efficient resource use, stable execution, and consistent Delta Lake + Kafka settings.  

Spark configuration options like `SPARK_UI_ENABLED`, `SPARK_UI_PORT`, `SPARK_PORT_MAX_RETRIES`, and `SPARK_LOCAL_IP` can be set via environment variables.


### Example usage:

```bash
from spark_session_manager import get_spark, stop_spark

# Get or create the shared SparkSession
spark = get_spark("LogStreamApp")

# Use Spark normally
df = spark.read.format("delta").load("/tmp/delta-lake/logs")
df.show()

# Stop the session explicitly when done
stop_spark()
```

This design guarantees that all Spark-based components in the platform run on a single, consistent session, making the pipeline more robust and easier to manage.

## Technologies

### Frontend

  - **React 18** with TypeScript
  - **Vite** for build and development
  - **shadcn/ui** for UI components
  - **Tailwind CSS** for styling
  - **Recharts** for interactive charts
  - **React Query** for state management

### Backend

  - **Node.js** with Express
  - **KafkaJS** for Kafka connectivity
  - **Server-Sent Events** for real-time streaming
  - **REST API** for queries and metrics

### Data Pipeline

  - **Apache Kafka** for message streaming
  - **Apache Spark** for processing
  - **Delta Lake** for storage
  - **Spark SQL** for querying

## Installation & Setup

### Prerequisites

  - Node.js 18+ and npm
  - Docker & Docker Compose (to start Kafka and related services)

### 1\. Clone the repository

```bash
git clone https://github.com/NicholasAtt/LogLake.git
cd LogLake
```

### 2\. Install dependencies

```bash
npm install
```

### 3\. Configure environment variables

```bash
cp .env.example .env
```

Edit the `.env` file with your configuration:

```env
# API Configuration
VITE_API_URL=http://localhost:4000

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=web-logs
KAFKA_GROUP_ID=ui-bridge-group

# Spark Configuration
SPARK_MASTER=spark://localhost:7077
DELTA_LAKE_PATH=/tmp/delta-lake

# Server Configuration
PORT=4000
```

### 4\. Start the backend server (optional)

```bash
npm run server
```

### 5\. Start the frontend app (optional)

```bash
npm run dev
```

-----

## Pipeline Configuration and Usage

### 1\. Docker Setup

The project uses `docker-compose.yml` to orchestrate Zookeeper, Kafka, and Kafka UI.  

All automation scripts are located in the `/scripts` folder:  

- **`setup_environment.sh`** – sets up the environment automatically.  
- **`start_pipeline.sh`** – starts the pipeline.  

With these scripts, you don't need to run Docker commands manually.


### 2\. `setup_environment.sh`

This script automates the initial setup. Run it once to:

  * Create necessary data and checkpoint directories.
  * Install Python dependencies from `requirements.txt`.
  * Download and configure Apache Spark.
  * Start the Docker services for Kafka and Zookeeper.
  * Automatically create the `web-logs` topic in Kafka.

Run the script from the terminal:

```bash
bash setup_environment.sh
```

### 3\. `start_pipeline.sh`

This is the main script to start the entire pipeline. It:

  * Checks for Docker and Docker Compose availability.
  * Starts the Kafka infrastructure services.
  * Sets up the Python environment and creates required directories.
  * Launches the main pipeline process in the background.
  * Provides access information for monitoring.
  * Includes a cleanup function to safely stop all processes with `Ctrl+C`.

Run the script from the terminal:

```bash
bash start_pipeline.sh
```


### A. Quick Start (recommended)

From the `scripts` directory:

```bash
pip install -r requirements.txt
./setup_environment.sh
python3 pipeline_orchestrator.py --action start
```

This will:

  - Start Kafka services with Docker Compose
  - Create Delta/Checkpoint directories
  - Train an anomaly detection model if not present
  - Start the streaming processor, ML streaming processor, and anomaly detector
  - Start the test log generator

To stop:

```bash
python3 pipeline_orchestrator.py --action stop
```

### B. Run components individually

In `scripts/`:

```bash
# 1) Generate logs
python3 log_generator.py --rate 10

# 2) Start rule-based streaming
python3 streaming_processor.py --mode stream --output-path /tmp/delta-lake/logs --checkpoint-path /tmp/checkpoints/logs

# 3) Start ML streaming (writes predictions separately)
python3 ml_streaming_processor.py --mode stream --output-path /tmp/delta-lake/logs --ml-output-path /tmp/delta-lake/ml-predictions --checkpoint-path /tmp/checkpoints/logs

# 4) Start anomaly detection
python3 anomaly_detector.py --mode detect --input-path /tmp/delta-lake/logs --output-path /tmp/delta-lake/anomalies --checkpoint-path /tmp/checkpoints/anomalies
```

### C. Analytics and maintenance

```bash
# Batch analytics on rule-based logs
python3 streaming_processor.py --mode analytics --output-path /tmp/delta-lake/logs

# ML analytics on predictions
python3 ml_streaming_processor.py --mode analytics --output-path /tmp/delta-lake/ml-predictions

# Analyze historical anomalies
python3 anomaly_detector.py --mode analyze --output-path /tmp/delta-lake/anomalies

# Optional: delta optimization
python3 streaming_processor.py --mode optimize --output-path /tmp/delta-lake/logs
```

### Python 3.11 Requirements

This project **requires Python 3.11** to ensure compatibility with all libraries, especially `kafka-python`.

If you are using **Python 3.12**, some libraries may not work correctly. In that case, make sure to update `pip`, `setuptools`, and `wheel`:

```bash
python3 -m ensurepip --upgrade
python3 -m pip install --upgrade pip setuptools wheel
```
Recommendation: The safest approach is to create a virtual environment with Python 3.11 (see below) and install dependencies there, avoiding conflicts with the system Python.

#### Creating a virtual environment with Python 3.11
```bash
# Create the virtual environment
python3.11 -m venv venv

# Activate it
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```
This ensures the full pipeline (log generator, streaming, ML, anomaly detection) runs correctly without compatibility issues.

### D. Spark configuration and ports

The scripts use a shared Spark session per process to avoid multiple SparkContext errors and disable the Spark UI by default. You can override with:

```bash
SPARK_UI_ENABLED=true SPARK_UI_PORT=4040 python3 ml_streaming_processor.py --mode stream
```

You can also increase port retries via `SPARK_PORT_MAX_RETRIES=64` and set `SPARK_LOCAL_IP`.

## Main Dashboard

### 1\. Main Dashboard

  - **Metrics Grid**: Real-time metrics (events/sec, error rate, response time, etc.)
  - **Analytics Charts**: Interactive charts for traffic and performance
  - **Live Log Stream**: Real-time log stream from the pipeline

### 2\. Query Interface

  - **Spark SQL**: Run custom queries on Delta Lake data
  - **Sample Queries**: Predefined queries for common analyses
  - **Real-time Results**: Live-updating results

### 3\. Monitoring

  - **Connection Status**: Visual indicators of pipeline status
  - **Anomaly Detection**: Automatic error and anomaly detection
  - **Performance Metrics**: Real-time performance monitoring

## Sample queries

### Error Analysis

```sql
SELECT endpoint, count(*) as error_count,
       avg(response_time) as avg_response_time
FROM delta_lake.logs
WHERE status >= 400 AND timestamp >= current_timestamp() - interval 1 hour
GROUP BY endpoint
ORDER BY error_count DESC
LIMIT 5
```

### User Session Analysis

```sql
SELECT
  user_id,
  count(distinct session_id) as sessions,
  count(*) as page_views,
  sum(response_time) / count(*) as avg_session_time
FROM delta_lake.logs
WHERE timestamp >= current_date()
GROUP BY user_id
ORDER BY page_views DESC
LIMIT 10
```

### Anomaly Detection

```sql
SELECT
  endpoint, source, level,
  count(*) as anomaly_count,
  max(response_time) as max_response_time
FROM delta_lake.logs
WHERE (level = 'ERROR' OR response_time > 1000)
  AND timestamp >= current_timestamp() - interval 30 minutes
GROUP BY endpoint, source, level
ORDER BY anomaly_count DESC
```

## Troubleshooting

### Connectivity Issues

1.  **Kafka unreachable**: Ensure Kafka runs on `localhost:9092`.
2.  **Spark unavailable**: Ensure Spark is active on `localhost:7077`.
3.  **Delta Lake inaccessible**: Verify permissions and configuration.

### Debug

  - Check server logs: `npm run server`
  - Verify SSE: `curl http://localhost:4000/health`
  - Test APIs: `curl http://localhost:4000/api/metrics`

## Metrics and performance

The platform monitors in real-time:

  - **Throughput**: Events processed per second
  - **Latency**: Average response time
  - **Error Rate**: Error percentage
  - **Active Sessions**: Active user sessions
  - **Data Processed**: Volume processed

## Contributing

1.  Fork the repository
2.  Create a feature branch (`git checkout -b feature/AmazingFeature`)
3.  Commit changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push the branch (`git push origin feature/AmazingFeature`)
5.  Open a Pull Request

## License

This project is licensed under the MIT License. See `LICENSE` for details.

## Support

For support and questions:

  - Open a GitHub issue
  - Contact the project maintainer
