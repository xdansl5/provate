# Real-Time Data Pipeline Platform

An interactive platform to ingest, process, and analyze real-time web logs using Kafka, Spark Structured Streaming, and Delta Lake. The repository includes scripts to run the end-to-end pipeline, optional ML-based anomaly detection, and a simple server for UI streaming.

## Key Features

### Real-Time Updates

  - **Live Log Stream**: Real-time view of logs generated by the pipeline
  - **Dynamic Metrics**: Automatic refresh of metrics based on real data
  - **Interactive Charts**: Charts update on new incoming data
  - **SSE Connection**: Server-Sent Events for instant updates

### Real Pipeline Data

  - **Kafka Integration**: Direct connection to Kafka topics
  - **Spark SQL Queries**: Execute queries against Delta Lake data
  - **Delta Lake Analytics**: Query Delta tables directly
  - **No Mock Data**: All data comes from the real pipeline

### Interactive Interface

  - **Connection Status**: Visual indicators of pipeline health
  - **Query Builder**: UI to run custom Spark SQL queries
  - **Advanced Filters**: Filter logs by level, source, endpoint
  - **Anomaly Detection**: Automatic anomaly detection on data

## Architecture

```
      Web Logs (Sources)
              |
              v
        Kafka (Broker)
              |
              v
  Spark Streaming (Processing)
              |
              v
      Delta Lake (Storage)
              |
              v
      API Server (Express)
              |
              v
    Web Platform (React App)
```

## Singleton Spark Session

The platform manages Spark with a singleton `SparkSession` (see `spark_session_manager.py`) to avoid multiple `SparkContext` errors (SPARK-2243).  

All pipeline components use `get_spark()` to share the same session, ensuring efficient resource use, stable execution, and consistent Delta Lake + Kafka settings.  

Spark configuration options like `SPARK_UI_ENABLED`, `SPARK_UI_PORT`, `SPARK_PORT_MAX_RETRIES`, and `SPARK_LOCAL_IP` can be set via environment variables.


### Example usage:

```bash
from spark_session_manager import get_spark, stop_spark

# Get or create the shared SparkSession
spark = get_spark("LogStreamApp")

# Use Spark normally
df = spark.read.format("delta").load("/tmp/delta-lake/logs")
df.show()

# Stop the session explicitly when done
stop_spark()
```

This design guarantees that all Spark-based components in the platform run on a single, consistent session, making the pipeline more robust and easier to manage.

## Technologies

### Frontend

  - **React 18** with TypeScript
  - **Vite** for build and development
  - **shadcn/ui** for UI components
  - **Tailwind CSS** for styling
  - **Recharts** for interactive charts
  - **React Query** for state management

### Backend

  - **Node.js** with Express
  - **KafkaJS** for Kafka connectivity
  - **Server-Sent Events** for real-time streaming
  - **REST API** for queries and metrics

### Data Pipeline

  - **Apache Kafka** for message streaming
  - **Apache Spark** for processing
  - **Delta Lake** for storage
  - **Spark SQL** for querying

## Installation & Setup

### Prerequisites

  - Node.js 22+ and npm
  - Docker & Docker Compose (to start Kafka and related services)

### 1\. Clone the repository

```bash
git clone https://github.com/NicholasAtt/LogLake.git
cd LogLake
```

### 2\. Install dependencies

```bash
npm install
```

### 3\. Configure environment variables

```bash
cp .env.example .env
```

Edit the `.env` file with your configuration:

```env
# API Configuration
VITE_API_URL=http://localhost:4000

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=web-logs
KAFKA_GROUP_ID=ui-bridge-group

# Spark Configuration
SPARK_MASTER=spark://localhost:7077
DELTA_LAKE_PATH=/tmp/delta-lake

# Server Configuration
PORT=4000
```

### 4\. Start the backend server (optional)

```bash
npm run server
```

This starts the Express backend which connects to Kafka (using `KAFKA_BROKERS` and `KAFKA_TOPIC` from your `.env`) and serves the SSE stream and API endpoints used by the UI. The backend expects the Kafka topic `web-logs` to already exist; if the topic is missing the server may log connection errors or fail to stream updates. Create the topic in your Kafka cluster (or enable topic auto-creation) before running the server.

### 5\. Start the frontend app (optional)

```bash
npm run dev
```

-----

## Pipeline Configuration and Usage

### 1. Docker Setup

The project uses `docker-compose.yml` to orchestrate Zookeeper, Kafka, and Kafka UI.

To build the containers, use the following command:

```bash
docker-compose build
```

### 2. Pipeline Usage

The Docker setup has been enhanced to streamline the deployment process and improve service management. Key improvements include:

- **Simplified Configuration**: The `docker-compose.yml` file has been optimized for easier customization and scalability.
- **Health Checks**: Added health checks for services to ensure they are running correctly before proceeding with operations.
- **Resource Management**: Improved resource allocation settings to optimize performance and reduce overhead.
- **Logging**: Enhanced logging capabilities for better monitoring and troubleshooting.

These improvements aim to provide a more robust and user-friendly experience when working with the real-time data pipeline.

### Starting the Pipeline

To start the services, run the following command:

```bash
docker-compose up -d
```

This command will start all the services defined in the `docker-compose.yml` file in detached mode.

Once the services are up and running, you can interact with Kafka and Zookeeper through their respective interfaces. For Kafka, you can use the command line tools or the Kafka UI provided by the Docker setup for monitoring and management.

### 3. Stopping the Pipeline

To stop the services, use the following command:

```bash
docker-compose down
```

This will safely shut down all running containers and clean up resources.

## Starting and Stopping the Pipeline

To start the pipeline, use the following command:

```bash
docker-compose exec spark-app python3 /app/scripts/pipeline_orchestrator.py --action start
```

This will:

- Start Kafka services with Docker Compose
- Create Delta/Checkpoint directories
- Train an anomaly detection model if not present
- Start the streaming processor, ML streaming processor, and anomaly detector
- Start the test log generator

## Running Components Individually

1) Generate logs:
```bash
docker-compose exec spark-app python3 /app/scripts/log_generator.py --rate 10
```

2) Process logs with streaming components:

a) Start basic rule-based streaming:
```bash
docker-compose exec spark-app python3 /app/scripts/streaming_processor.py --mode stream --kafka-servers kafka:29092 --topic web-logs
```

b) Start ML-powered streaming:
```bash
docker-compose exec spark-app python3 /app/scripts/ml_streaming_processor.py --mode stream --kafka-servers kafka:29092 --topic web-logs
```

c) Start anomaly detection:
```bash
docker-compose exec spark-app python3 /app/scripts/anomaly_detector.py --mode detect
```

3) Run analytics and maintenance:

a) Analyze historical logs:
```bash
docker-compose exec spark-app python3 /app/scripts/streaming_processor.py --mode analytics 
```

b) Run ML analytics:
```bash
docker-compose exec spark-app python3 /app/scripts/ml_streaming_processor.py --mode analytics 
```

c) Analyze historical anomalies:
```bash
docker-compose exec spark-app python3 /app/scripts/anomaly_detector.py --mode analyze 
```

d) Optimize Delta Lake tables:
```bash
docker-compose exec spark-app python3 /app/scripts/streaming_processor.py --mode optimize
```

4) ML model management:

Train a new anomaly detection model:
```bash
docker-compose exec spark-app python3 /app/scripts/streaming_processor.py --mode stream --output-path /tmp/delta-lake/logs --checkpoint-path /tmp/checkpoints/logs
```

3) Start ML streaming (writes predictions separately):
```bash
docker-compose exec spark-app python3 /app/scripts/ml_streaming_processor.py --mode stream --output-path /tmp/delta-lake/logs --ml-output-path /tmp/delta-lake/ml-predictions --checkpoint-path /tmp/checkpoints/logs
```

4) Start anomaly detection:
```bash
docker-compose exec spark-app python3 /app/scripts/anomaly_detector.py --mode detect --input-path /tmp/delta-lake/logs --output-path /tmp/delta-lake/anomalies --checkpoint-path /tmp/checkpoints/anomalies
```

## Analytics and Maintenance

- Batch analytics on rule-based logs:
```bash
docker-compose exec spark-app python3 /app/scripts/streaming_processor.py --mode analytics --output-path /tmp/delta-lake/logs
```

- ML analytics on predictions:
```bash
docker-compose exec spark-app python3 /app/scripts/ml_streaming_processor.py --mode analytics --output-path /tmp/delta-lake/ml-predictions 
```

- Analyze historical anomalies:
```bash
docker-compose exec spark-app python3 /app/scripts/anomaly_detector.py --mode analyze --output-path /tmp/delta-lake/anomalies
```

- Optional: delta optimization:
```bash
docker-compose exec spark-app python3 /app/scripts/streaming_processor.py --mode optimize --output-path /tmp/delta-lake/logs
```

## Troubleshooting

### Connectivity Issues

1.  **Kafka unreachable**: Ensure Kafka runs on `localhost:9092`.
2.  **Spark unavailable**: Ensure Spark is active on `localhost:7077`.
3.  **Delta Lake inaccessible**: Verify permissions and configuration.

### Debug

  - Check server logs: `npm run server`
  - Verify SSE: `curl http://localhost:4000/health`
  - Test APIs: `curl http://localhost:4000/api/metrics`

## Metrics and performance

The platform monitors in real-time:

  - **Throughput**: Events processed per second
  - **Latency**: Average response time
  - **Error Rate**: Error percentage
  - **Active Sessions**: Active user sessions
  - **Data Processed**: Volume processed

## Contributing

1.  Fork the repository
2.  Create a feature branch (`git checkout -b feature/AmazingFeature`)
3.  Commit changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push the branch (`git push origin feature/AmazingFeature`)
5.  Open a Pull Request

## License

This project is licensed under the MIT License. See `LICENSE` for details.

## Support

For support and questions:

  - Open a GitHub issue
  - Contact the project maintainer